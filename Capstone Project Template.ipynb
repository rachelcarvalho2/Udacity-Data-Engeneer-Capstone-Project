{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project's main function is to put into practice all the concepts learned in this course. For this, I94 immigration data will be integrated with US Cities demographics data and airport data.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, when, count, udf, isnan, year, month, date_format\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This project will integrate I94 immigration data, airport codes and US cities demographics. For this integration, the star schema model will be used.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Data Sets:\n",
    "- I94 Imigration Data\n",
    "- Airport Codes\n",
    "- US Cities Demographics\n",
    "\n",
    "AWS S3 will be used for data storage and Pyspark for data processing and exploitation.\n",
    "\n",
    "- I94 Immigration Data: This data comes from the US National Tourism and Trade Office. \n",
    "\n",
    "- US Cities Demographics: This data comes from OpenSoft.\n",
    "\n",
    "- Airport Codes: This is a simple table of airport codes and corresponding cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Immigration Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "#write to parquet\n",
    "df_immigration=spark.read.parquet(\"sas_data\")\n",
    "df_immigration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Demographics Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(City='Silver Spring', State='Maryland', Median Age=33.8, Male Population=40601, Female Population=41862, Total Population=82463, Number of Veterans=1562, Foreign-born=30908, Average Household Size=2.6, State Code='MD', Race='Hispanic or Latino', Count=25924)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics=spark.read.csv('us-cities-demographics.csv', inferSchema=True, header=True, sep=';')\n",
    "df_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Airport Codes Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ident='00A', type='heliport', name='Total Rf Heliport', elevation_ft=11, continent='NA', iso_country='US', iso_region='US-PA', municipality='Bensalem', gps_code='00A', iata_code=None, local_code='00A', coordinates='-74.93360137939453, 40.07080078125')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport = spark.read.csv('airport-codes_csv.csv', inferSchema=True, header=True)\n",
    "df_airport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Print Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+--------------------+-----------------+---------------+--------------------+-------+-----+--------------------+-----------------+-----------------+--------------------+-----------------+-----------------+-----------------+--------------------+--------------------+------------------+-----------------+-----------------+------+------------------+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|             i94mode|          i94addr|        depdate|              i94bir|i94visa|count|            dtadfile|         visapost|            occup|             entdepa|          entdepd|          entdepu|          matflag|             biryear|             dtaddto|            gender|           insnum|          airline|admnum|             fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+--------------------+-----------------+---------------+--------------------+-------+-----+--------------------+-----------------+-----------------+--------------------+-----------------+-----------------+-----------------+--------------------+--------------------+------------------+-----------------+-----------------+------+------------------+--------+\n",
      "|  0.0|  0.0|   0.0|   0.0|   0.0|    0.0|    0.0|0.007718857880324115|4.928183940060324|4.6008591508675|0.025901774142342845|    0.0|  0.0|3.229647648671178E-5|60.75774639062653|99.73755883206898|0.007686561403837...|4.470768943579024|99.98733978121722|4.470768943579024|0.025901774142342845|0.015405419284161517|13.379429017673601|96.32763225164898|2.700857439154246|   0.0|0.6313638188387285|     0.0|\n",
      "+-----+-----+------+------+------+-------+-------+--------------------+-----------------+---------------+--------------------+-------+-----+--------------------+-----------------+-----------------+--------------------+-----------------+-----------------+-----------------+--------------------+--------------------+------------------+-----------------+-----------------+------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_immigration = df_immigration.select([(count(when(isnan(c) | col(c).isNull(), c))*100/df_immigration.count()).alias(c) for c in df_immigration.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------------------+-------------------+----------------+-------------------+-------------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|    Male Population|  Female Population|Total Population| Number of Veterans|       Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+-------------------+-------------------+----------------+-------------------+-------------------+----------------------+----------+----+-----+\n",
      "| 0.0|  0.0|       0.0|0.10377032168799723|0.10377032168799723|             0.0|0.44967139398132133|0.44967139398132133|    0.5534417156693185|       0.0| 0.0|  0.0|\n",
      "+----+-----+----------+-------------------+-------------------+----------------+-------------------+-------------------+----------------------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_demographics = df_demographics.select([(count(when(isnan(c) | col(c).isNull(), c))*100/df_demographics.count()).alias(c) for c in df_demographics.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+----------------+---------+-----------+----------+------------------+------------------+-----------------+-----------------+-----------+\n",
      "|ident|type|name|    elevation_ft|continent|iso_country|iso_region|      municipality|          gps_code|        iata_code|       local_code|coordinates|\n",
      "+-----+----+----+----------------+---------+-----------+----------+------------------+------------------+-----------------+-----------------+-----------+\n",
      "|  0.0| 0.0| 0.0|12.7208352246936|      0.0|        0.0|       0.0|10.305946436677258|25.501588742623696|83.31547889241943|47.91466182478438|        0.0|\n",
      "+-----+----+----+----------------+---------+-----------+----------+------------------+------------------+-----------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_airport = df_airport.select([(count(when(isnan(c) | col(c).isNull(), c))*100/df_airport.count()).alias(c) for c in df_airport.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "duplicated data and columns with more than 80% null data will be deleted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration=df_immigration.dropDuplicates().drop(*['occup', 'entdepu','insnum'])\n",
    "\n",
    "@udf(StringType())\n",
    "def convert_datetime(x):\n",
    "    if x:\n",
    "        return (datetime(1960, 1, 1).date() + timedelta(x)).isoformat()\n",
    "    return None\n",
    "\n",
    "df_immigration=df_immigration.withColumn(\"arrdate\", convert_datetime(df_immigration.arrdate))\n",
    "df_immigration=df_immigration.withColumnRenamed(\"cicid\",\"immigration_id\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"i94yr\",\"year\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"i94mon\",\"month\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"i94cit\",\"country_code_1\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"i94res\",\"country_code_2\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"i94port\",\"city_code\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"arrdate\",\"arrival_date\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"i94mode\",\"transportation_type\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"i94addr\",\"state_code\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"depdate\",\"departure_date\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"i94bir\",\"respondent_age\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"i94visa\",\"visa_code\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"dtadfile\",\"file_date\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"visapost\",\"state_department\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"entdepa\",\"arrival_flag\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"entdepd\",\"departure_flag\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"matflag\",\"match_flag\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"biryear\",\"birth_year\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"dtaddto\",\"admitted_date\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"admnum\",\"admission_number\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"fltno\",\"flight_number\")\n",
    "df_immigration=df_immigration.withColumnRenamed(\"visatype\",\"admission_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport=df_airport.dropDuplicates().drop(*['iata_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics=df_demographics.dropDuplicates()\n",
    "\n",
    "df_demographics=df_demographics.withColumnRenamed(\"Median Age\",\"median_age\")\n",
    "df_demographics=df_demographics.withColumnRenamed(\"Male Population\",\"male_population\")\n",
    "df_demographics=df_demographics.withColumnRenamed(\"Female Population\",\"female_population\")\n",
    "df_demographics=df_demographics.withColumnRenamed(\"Total Population\",\"total_population\")\n",
    "df_demographics=df_demographics.withColumnRenamed(\"Number of Veterans\",\"number_veterans\")\n",
    "df_demographics=df_demographics.withColumnRenamed(\"Foreign-born\",\"foreign_born\")\n",
    "df_demographics=df_demographics.withColumnRenamed(\"Average Household Size\",\"average_household_size\")\n",
    "df_demographics=df_demographics.withColumnRenamed(\"State Code\",\"state_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The data model used will be the star schema.\n",
    "\n",
    "Fact Table\n",
    "- Fact_Immigration\\\n",
    "immigration_id (PK)\\\n",
    "country_code_1\\\n",
    "country_code_2\\\n",
    "city_code\\\n",
    "state_code(FK)\\\n",
    "count\\\n",
    "arrival_date(FK)\\\n",
    "departure_date\\\n",
    "admission_number\\\n",
    "flight_number\\\n",
    "admission_class\n",
    "\n",
    "Dimension Tables\n",
    "\n",
    "- Dim_Demographics\\\n",
    "city\\\n",
    "State\\\n",
    "median_age\\\n",
    "male_population\\\n",
    "female_population\\\n",
    "total_population\\\n",
    "number_veterans\\\n",
    "foreign_born\\\n",
    "average_household_size\\\n",
    "state_code(Pk)\\\n",
    "race\\\n",
    "count\n",
    "\n",
    "\n",
    "- Dim_Airport\\\n",
    "ident(PK)\\\n",
    "type\\\n",
    "name\\\n",
    "elevation_ft\\\n",
    "continent\\\n",
    "iso_country\\\n",
    "iso_region\\\n",
    "municipality\\\n",
    "gps_code\\\n",
    "local_code\\\n",
    "coordinates\n",
    "\n",
    "\n",
    "- Dim_Person\\\n",
    "immigration_id(PK)\\\n",
    "gender\\\n",
    "birth_year\\\n",
    "visa_code\n",
    "\n",
    "\n",
    "- Dim_arrival_calendar\\\n",
    "date(PK)\\\n",
    "year\\\n",
    "month\\\n",
    "day\\\n",
    "week\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1- Create Temp View of immigration table to populate immigration fact table, and arrival calendar and person dimension tables\\\n",
    "2- Write Parquets for demografics and airport dimension tables\\\n",
    "3- Select data for arrival calendar table and Write Parquet\\\n",
    "4- Select data for person table and Write Parquet\\\n",
    "5- Select data for immigration table and Write Parquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/workspace/dim_demographics already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o663.parquet.\n: org.apache.spark.sql.AnalysisException: path file:/home/workspace/dim_demographics already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0d5251f49ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_demographics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dim_demographics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_airport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dim_airport\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/workspace/dim_demographics already exists.;'"
     ]
    }
   ],
   "source": [
    "df_demographics.write.parquet(\"dim_demographics\")\n",
    "df_airport.write.parquet(\"dim_airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_calendar=df_immigration.select(\"arrival_date\")\n",
    "df_calendar=df_calendar.withColumnRenamed(\"arrival_date\",\"date\")\n",
    "df_calendar=df_calendar.withColumn(\"year\",year(df_calendar.date))\n",
    "df_calendar=df_calendar.withColumn(\"month\",month(df_calendar.date))\n",
    "df_calendar=df_calendar.withColumn(\"week\",date_format(col(\"date\"),\"w\"))\n",
    "df_calendar=df_calendar.withColumn(\"day\",date_format(col(\"date\"),\"d\"))\n",
    "df_calendar=df_calendar.dropDuplicates()\n",
    "df_calendar.write.partitionBy(\"year\",\"month\").parquet(\"dim_arrival_calendar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_person=df_immigration.select(\"immigration_id\",\"gender\",\"birth_year\",\"visa_code\").dropDuplicates()\n",
    "df_person.write.parquet(\"dim_person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration=df_immigration.select(\"immigration_id\",\"country_code_1\",\"country_code_2\",\"city_code\",\"state_code\",\"count\", \"arrival_date\", \"departure_date\", \"admission_number\", \"flight_number\", \"admission_class\").dropDuplicates()\n",
    "df_immigration.write.partitionBy(\"country_code_1\",\"arrival_date\").parquet(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "To check the quality of the data, select count(*), count distinct PK and count PK will be done to confirm that we have data in the dimension and fact tables and that there is no duplicate data.\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create Temp Tables for checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics.createOrReplaceTempView(\"dim_demographics\")\n",
    "df_airport.createOrReplaceTempView(\"dim_airport\")\n",
    "df_arrival_calendar.createOrReplaceTempView(\"dim_arrival_calendar\")\n",
    "df_person.createOrReplaceTempView(\"dim_person\")\n",
    "df_immigration.createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "count(*)\n",
    ", count(distinct state_code)\n",
    ", count(state_code)\n",
    "from dim_demographics\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*)\n",
    ", count(distinct ident)\n",
    ", count(ident)\n",
    "from dim_airport\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*)\n",
    ", count(distinct date)\n",
    ", count(date)\n",
    "from dim_arrival_calendar\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*)\n",
    ", count(distinct imigration_id)\n",
    ", count(imigration_id)\n",
    "from dim_person\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*)\n",
    ", count(distinct imigration_id)\n",
    ", count(imigration_id)\n",
    "from fact_immigration\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we are dealing with long and historical data, it is essential to use spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Propose how often the data should be updated and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The automation of the process depends on the frequency of reporting data and the frequency of updating the databases. Analyzing this data, it is estimated that they are reported and updated once a month, therefore, the suggested automation would be once a month, 1 day before the report and on the day of the base update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we are already using a system that supports bigdata, the only difference would be to add more processing time before reporting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this case, it would be necessary to understand the time that the base is updated, so, at the end of this time, the pipeline would be updated soon afterwards to have plenty of time to correct possible errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For this, it is necessary to add these tables in a data warehouse or lake, so that the users will be able to access the star schema without problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
